# src/models.py
import torch
import torch.nn as nn
import numpy as np

class SinusoidalPosEmb(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, x):
        device = x.device
        half_dim = self.dim // 2
        emb = np.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)
        emb = x * emb[None, :]
        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)
        return emb

class VectorFieldNet(nn.Module):
    """
    æ¡ä»¶æµåŒ¹é…ç½‘ç»œ v_theta(x_t, t, x_0)
    """
    def __init__(self, input_dim, hidden_dim=256, dropout=0.1):
        super().__init__()
        # æ—¶é—´åµŒå…¥
        self.time_mlp = nn.Sequential(
            SinusoidalPosEmb(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim),
            nn.SiLU(),
            nn.Linear(hidden_dim, hidden_dim),
        )
        # è¾“å…¥æŠ•å½± (x_t, x_0 æ‹¼æ¥)
        self.input_proj = nn.Linear(input_dim * 2, hidden_dim)
        
        # ä¸»å¹²ç½‘ç»œ (Residual MLP)
        self.blocks = nn.ModuleList([ResidualBlock(hidden_dim, dropout=dropout) for _ in range(6)])
        
        # SDE è§£è€¦è¾“å‡ºå±‚
        self.mu_head = nn.Linear(hidden_dim, input_dim) # Drift mu
        self.sigma_head = nn.Linear(hidden_dim, 1)      # Log-sigma
        
        # åˆå§‹åŒ– sigma head ä½¿å…¶åˆå§‹å€¼åå° (bias < 0)
        nn.init.constant_(self.sigma_head.bias, -2.0)

    def forward(self, x, t, x_0):
        """
        Args:
            x: å½“å‰çŠ¶æ€ [B, D]
            t: æ—¶é—´ [B, 1]
            x_0: åˆå§‹çŠ¶æ€ [B, D]
        """
        t_emb = self.time_mlp(t)
        x_input = torch.cat([x, x_0], dim=-1)
        x_emb = self.input_proj(x_input)
        h = x_emb + t_emb
        for block in self.blocks:
            h = block(h)
        
        mu_pred = self.mu_head(h)
        log_sigma_pred = self.sigma_head(h)
        # é™åˆ¶ log_sigma èŒƒå›´ï¼Œé˜²æ­¢æ•°å€¼çˆ†ç‚¸
        # ğŸ”‘ ç¨å¾®æ”¾å®½ä¸Šé™ï¼Œå…è®¸æ¨¡å‹åœ¨éœ€è¦çš„åœ°æ–¹æœ‰æ›´å¤§çš„æ¢ç´¢æ€§
        log_sigma_pred = torch.clamp(log_sigma_pred, min=-10.0, max=3.0)
        return mu_pred, log_sigma_pred


class ResidualBlock(nn.Module):
    def __init__(self, dim, dropout=0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, dim),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Linear(dim, dim),
            nn.Dropout(dropout),
        )
        self.act = nn.SiLU()

    def forward(self, x):
        return self.act(x + self.net(x))