# main.py
import argparse
import design_bench
import torch
import torch.optim as optim
import numpy as np

from src.config import Config, load_config
from src.utils import set_seed, Normalizer
from src.oracle import NTKOracle
from src.generator import GP, sampling_data_from_GP, generate_trajectories_from_GP_samples,RFFGP
from src.models import VectorFieldNet
from src.flow import train_cfm_step,train_cfm, inference_ode
import time
import os


def parse_args():
    parser = argparse.ArgumentParser(description="GGFM training")
    parser.add_argument(
        "-c",
        "--config",
        type=str,
        default="configs/TfBind8_FlowMatching.yaml",
        help="Path to config yaml",
    )
    parser.add_argument(
        "-s",
        "--seed",
        type=int,
        default=None,
        help="Override random seed",
    )
    parser.add_argument(
        "--device",
        type=str,
        default=None,
        help="Override device (e.g. cuda, cpu)",
    )
    return parser.parse_args()


def resolve_config_path(config_path):
    if os.path.isabs(config_path) or os.path.exists(config_path):
        return config_path
    return os.path.join(os.path.dirname(__file__), config_path)

def get_design_bench_data(task_name):
    """
    åŠ è½½å¹¶æ ‡å‡†åŒ– Design-Bench æ•°æ®ï¼Œæ”¯æŒç¦»æ•£ä»»åŠ¡è½¬æ¢
    å®Œå…¨å¯¹é½ ROOT çš„å¤„ç†æ–¹å¼
    """
    print(f"Loading task: {task_name}...")
    if task_name != 'TFBind10-Exact-v0':
        task = design_bench.make(task_name)
    else:
        # æ˜¾å­˜ä¼˜åŒ–ï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
        task = design_bench.make(task_name, dataset_kwargs={"max_samples": 10000})
    
    offline_x = task.x
    logits_shape = None  # ä¿å­˜ logits å½¢çŠ¶ä¿¡æ¯
    
    if task.is_discrete:
        # ROOT é£æ ¼ï¼šä½¿ç”¨ map_to_logits ä¿®æ”¹ task å†…éƒ¨çŠ¶æ€
        # è¿™æ · task.predict() æ‰èƒ½æ­£ç¡®å¤„ç† logits æ ¼å¼çš„æ•°æ®
        task.map_to_logits()
        offline_x = task.x  # ç°åœ¨ task.x å·²ç»æ˜¯ logits æ ¼å¼ (N, L, V-1)
        logits_shape = offline_x.shape  # ä¿å­˜å½¢çŠ¶ (N, L, V-1)
        offline_x = offline_x.reshape(offline_x.shape[0], -1)  # å±•å¹³ä¸º (N, L*(V-1))
        print(f"[æ•°æ®ç¼–ç ] ç¦»æ•£ä»»åŠ¡ï¼šå·²è°ƒç”¨ map_to_logitsï¼ŒLogits {logits_shape} -> å±•å¹³ {offline_x.shape}")
    else:
        print("[æ•°æ®ç¼–ç ] è¿ç»­ä»»åŠ¡ï¼šç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®")
    
    # è®¡ç®—ç»Ÿè®¡é‡ï¼ˆä¸ ROOT å®Œå…¨ä¸€è‡´ï¼‰
    mean_x = np.mean(offline_x, axis=0)
    std_x = np.std(offline_x, axis=0)
    std_x = np.where(std_x == 0, 1.0, std_x)  # ROOT ä½¿ç”¨ == 0ï¼Œä¸æ˜¯ < 1e-6
    offline_x_norm = (offline_x - mean_x) / std_x
    
    # å¤„ç† Yï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
    offline_y = task.y.reshape(-1)  # ROOT ä½¿ç”¨ reshape(-1)ï¼Œä¸æ˜¯ reshape(-1, 1)
    mean_y = np.mean(offline_y, axis=0)
    std_y = np.std(offline_y, axis=0)
    
    # æ´—ç‰Œæ•°æ®ï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
    shuffle_idx = np.random.permutation(offline_x.shape[0])
    offline_x_norm = offline_x_norm[shuffle_idx]
    offline_y = offline_y[shuffle_idx]
    
    # æ ‡å‡†åŒ– Y
    offline_y_norm = (offline_y - mean_y) / std_y
    
    return task, offline_x_norm, offline_y_norm, mean_x, std_x, mean_y, std_y, logits_shape

# main.py é¢„å¤„ç†é€»è¾‘
# def preprocess_trajectories(oracle, X_train_norm):
#     device = torch.device(Config.DEVICE if torch.cuda.is_available() else "cpu")
#     if os.path.exists(Config.TRAJECTORY_PATH):
#         print(f"Loading cached trajectories from {Config.TRAJECTORY_PATH}")
#         return np.load(Config.TRAJECTORY_PATH)['trajs']

#     print("=== Generating ALL long trajectories (GD reverse + GA) ===")
#     all_indices = np.arange(len(X_train_norm))
#     # æŒ‰ç…§ä½ çš„è¦æ±‚ï¼šç”¨å…¨é‡æ•°æ®ï¼Œæˆ–è€…è‡³å°‘ 10000 æ¡
#     sample_size = len(all_indices)
#     selected_idx = all_indices
#     # selected_idx = np.random.choice(all_indices, sample_size, replace=False)
    
#     all_valid = []
#     batch_size = 256
#     for i in range(0, sample_size, batch_size):
#         batch_x = X_train_norm[selected_idx[i : i + batch_size]]
#         trajs = generate_long_trajectories(oracle, batch_x, device)
#         all_valid.append(trajs)
#         print(f"Progress: {i + batch_size}/{sample_size}")
        
#     pool = np.concatenate(all_valid, axis=0)
#     np.savez_compressed(Config.TRAJECTORY_PATH, trajs=pool)
#     return pool

def main():
    args = parse_args()
    config_path = resolve_config_path(args.config)
    load_config(config_path)
    if args.seed is not None:
        Config.SEED = args.seed
    if args.device is not None:
        Config.DEVICE = args.device

    # 0. åˆå§‹åŒ–ç¯å¢ƒ
    print(f"=== GGFM with ROOT GP Sampling: {Config.TASK_NAME} ===")
    print(f"[Config] Using: {config_path}")
    set_seed(Config.SEED)
    device = torch.device(Config.DEVICE if torch.cuda.is_available() else "cpu")
    
    # 1. åŠ è½½å¹¶ç¼–ç æ•°æ®ï¼ˆå®Œå…¨å¯¹é½ ROOT çš„å¤„ç†æ–¹å¼ï¼‰
    task, X_train_norm, y_train_norm, mean_x, std_x, mean_y, std_y, logits_shape = get_design_bench_data(Config.TASK_NAME)
    
    # åŒæ­¥ Normalizer çŠ¶æ€
    x_normalizer = Normalizer(np.zeros((1, X_train_norm.shape[1])))
    x_normalizer.mean, x_normalizer.std, x_normalizer.device = mean_x, std_x, device
    
    # 2. è½¬æ¢ä¸º Tensor ä¾› GP ä½¿ç”¨ï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
    X_train_tensor = torch.FloatTensor(X_train_norm).to(device)
    # y_train_norm ç°åœ¨æ˜¯ (N,) å½¢çŠ¶ï¼Œéœ€è¦è½¬æ¢ä¸º (N, 1) ä¾› Oracle ä½¿ç”¨
    y_train_tensor = torch.FloatTensor(y_train_norm).reshape(-1, 1).to(device)
    
    # ä¿å­˜åŸå§‹ç»Ÿè®¡é‡ï¼ˆç”¨äºåæ ‡å‡†åŒ–ï¼‰
    mean_x_torch = torch.FloatTensor(mean_x).to(device)
    std_x_torch = torch.FloatTensor(std_x).to(device)
    mean_y_torch = torch.FloatTensor([mean_y]).to(device)
    std_y_torch = torch.FloatTensor([std_y]).to(device)
    
    # 3. åˆå§‹åŒ– GP è¶…å‚æ•°
    lengthscale = torch.tensor(Config.GP_INITIAL_LENGTHSCALE, device=device)
    variance = torch.tensor(Config.GP_INITIAL_OUTPUTSCALE, device=device)
    noise = torch.tensor(Config.GP_NOISE, device=device)
    mean_prior = torch.tensor(0.0, device=device)
    
    # 4. é€‰æ‹©ç”¨äº GP æ‹Ÿåˆçš„åˆå§‹ç‚¹ï¼ˆå®Œå…¨å¯¹é½ ROOTï¼‰
    if Config.GP_TYPE_INITIAL_POINTS == 'highest':
        # ROOT: å›ºå®š 1024 ä¸ªæ ·æœ¬ï¼Œæ¯æ¬¡å…¨é€‰ä½†é¡ºåºä¸åŒ
        best_indices = torch.argsort(y_train_tensor.view(-1))[-1024:]
        best_x = X_train_tensor[best_indices]
        print(f"[GP Init] Using top 1024 samples for GP sampling (ROOT style: same samples, different order each epoch)")
    elif Config.GP_TYPE_INITIAL_POINTS == 'lowest':
        best_indices = torch.argsort(y_train_tensor.view(-1))[:1024]
        best_x = X_train_tensor[best_indices]
        print(f"[GP Init] Using bottom 1024 samples for GP sampling")
    else:
        best_x = X_train_tensor
        print(f"[GP Init] Using all samples for GP sampling")
    # 1. é¢„å…ˆå‡†å¤‡å¥½å…¨é‡ç´¢å¼• (åœ¨å¾ªç¯å¤–)
    all_indices = torch.arange(X_train_tensor.shape[0], device=device)
    # é¢„å…ˆè®¡ç®—å¥½é«˜åˆ†ç´¢å¼• (Top 2000 ä½œä¸ºä¸€ä¸ªæ± å­ï¼Œé˜²æ­¢åªç›¯ç€ Top 1024 è¿‡æ‹Ÿåˆ)
    top_k_indices = torch.argsort(y_train_tensor.view(-1), descending=True)[:2000]
    
    # 5. åˆå§‹åŒ– Flow Matching ç½‘ç»œ
    input_dim = X_train_norm.shape[1]
    cfm_model = VectorFieldNet(
        input_dim,
        hidden_dim=Config.HIDDEN_DIM,
        dropout=Config.DROPOUT,
    ).to(device)
    optimizer = optim.Adam(cfm_model.parameters(), lr=Config.FM_LR)
    
    # ğŸ”§ æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œé˜²æ­¢åæœŸå­¦ä¹ ç‡è¿‡å¤§
    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.FM_EPOCHS, eta_min=1e-5)

    # --- æ ¸å¿ƒä¿®æ”¹ï¼šæ¯ä¸ª Epoch åŠ¨æ€é‡‡æ · GP å‡½æ•°ç”Ÿæˆè½¨è¿¹ ---
    print(f"=== Training: Dynamic GP Sampling ({Config.FM_EPOCHS} Epochs) ===")
    print(f"æ¯ä¸ª Epoch é‡‡æ · n_e = {Config.GP_NUM_FUNCTIONS} ä¸ª GP å‡½æ•°")
    print(f"æ¯ä¸ª GP å‡½æ•°é‡‡æ · {Config.GP_NUM_POINTS} ä¸ªé…å¯¹")
    print(f"æ€»è®¡å°†ç”Ÿæˆçº¦ {Config.GP_NUM_FUNCTIONS} Ã— {Config.FM_EPOCHS} = {Config.GP_NUM_FUNCTIONS * Config.FM_EPOCHS} ä¸ª GP å‡½æ•°")

    # y_train_norm å·²ç»æ˜¯ (N,) å½¢çŠ¶äº†ï¼Œä¸éœ€è¦ flatten
    y_scores_flat = y_train_norm

    for epoch in range(Config.FM_EPOCHS):
        # æ¯ä¸ª Epoch é‡æ–°é‡‡æ ·å…·æœ‰ä¸åŒè¶…å‚æ•°çš„ GP
        epoch_start = time.time()  # è®°å½• epoch å¼€å§‹æ—¶é—´
        print(f"\n=== Epoch {epoch+1}/{Config.FM_EPOCHS} ===")

        # === æ ¸å¿ƒä¿®æ”¹ï¼šæ„å»ºæ··åˆé‡‡æ ·æ±  (Mix 50/50) ===
        # A. 50% æ¥è‡ª Top é«˜åˆ† (ä¿è¯ä¸Šé™)
        num_high = int(Config.GP_NUM_POINTS // 2)  # ä¾‹å¦‚ 512
        # ä» Top 2000 é‡Œéšæœºé€‰ 512 ä¸ªï¼Œå¢åŠ ä¸€ç‚¹å˜å¼‚æ€§
        idx_high = top_k_indices[torch.randperm(len(top_k_indices))[:num_high]]
        
        # B. 50% æ¥è‡ªå…¨å±€éšæœº (ä¿è¯ä¸­ä½æ•°å’Œæ³›åŒ–)
        num_rand = Config.GP_NUM_POINTS - num_high
        idx_rand = all_indices[torch.randperm(len(all_indices))[:num_rand]]
        
        # C. åˆå¹¶
        mixed_indices = torch.cat([idx_high, idx_rand])
        current_epoch_x = X_train_tensor[mixed_indices]
        
        # -------------------------------------------------
        
        # æ„å»º GP æ¨¡å‹ï¼ˆTFBind8 ä½¿ç”¨éƒ¨åˆ†æ ·æœ¬ï¼Œä¸ ROOT ä¸€è‡´ï¼‰
        gp_init_start = time.time()
        # if Config.TASK_NAME == 'TFBind8-Exact-v0':
        #     selected_fit_samples = torch.randperm(X_train_tensor.shape[0])[:Config.GP_NUM_FIT_SAMPLES]
        #     GP_Model = GP(
        #         device=device,
        #         x_train=X_train_tensor[selected_fit_samples],
        #         y_train=y_train_tensor[selected_fit_samples].view(-1),  # ç¡®ä¿æ˜¯ (N,) å½¢çŠ¶
        #         lengthscale=lengthscale,
        #         variance=variance,
        #         noise=noise,
        #         mean_prior=mean_prior
        #     )
        # else:
        # GP_Model = GP(
        #     device=device,
        #     x_train=X_train_tensor,
        #     y_train=y_train_tensor.view(-1),  # ç¡®ä¿æ˜¯ (N,) å½¢çŠ¶
        #     lengthscale=lengthscale,
        #     variance=variance,
        #     noise=noise,
        #     mean_prior=mean_prior
        # )
        GP_Model = RFFGP(
                device=device,
                x_train=X_train_tensor, 
                y_train=y_train_tensor.view(-1),  # ç¡®ä¿æ˜¯ (N,) å½¢çŠ¶, 
                lengthscale=lengthscale, 
                variance=variance, 
                noise=noise,
                num_features=2048 
            )
        gp_init_time = time.time() - gp_init_start
        
        # ä» GP é‡‡æ · n_e = 8 ä¸ªå‡½æ•°ï¼Œæ¯ä¸ªå‡½æ•°ç”Ÿæˆ num_points ä¸ªé…å¯¹
        # best_x = torch.randperm(X_train_tensor.shape[0])[:1024]
        sampling_start = time.time()
        data_from_GP = sampling_data_from_GP(
            x_train=current_epoch_x,
            device=device,
            GP_Model=GP_Model,
            num_functions=Config.GP_NUM_FUNCTIONS,
            num_gradient_steps=Config.GP_NUM_GRADIENT_STEPS,
            num_points=Config.GP_NUM_POINTS,
            learning_rate=Config.GP_LEARNING_RATE,
            delta_lengthscale=Config.GP_DELTA_LENGTHSCALE,
            delta_variance=Config.GP_DELTA_VARIANCE,
            seed=epoch,  # ä½¿ç”¨ epoch ä½œä¸ºéšæœºç§å­ï¼Œç¡®ä¿æ¯ä¸ª epoch ä¸åŒ
            threshold_diff=Config.GP_THRESHOLD_DIFF,
            uncertainty_penalty=Config.GP_UNCERTAINTY_PENALTY,
            uncertainty_interval=Config.GP_UNCERTAINTY_INTERVAL,
            max_end_uncertainty=Config.GP_MAX_END_UNCERTAINTY,
            verbose=(epoch == 0)  # ç¬¬ä¸€ä¸ª epoch æ˜¾ç¤ºè¯¦ç»†è®¡æ—¶
        )
        sampling_time = time.time() - sampling_start
        
        # ä» GP é‡‡æ ·ç»“æœç”Ÿæˆè½¨è¿¹
        traj_gen_start = time.time()
        trajs_array, scores_array = generate_trajectories_from_GP_samples(
            data_from_GP,
            device=device,
            num_steps=Config.GP_TRAJ_STEPS
        )
        # # 2. è®¡ç®—åŠ¨æ€ Softmax æƒé‡ (Dynamic Softmax Weighting)
        # if len(scores_array) > 0:
        #     # è½¬ä¸º Tensor
        #     batch_scores = torch.FloatTensor(scores_array).to(device)
            
        #     # === å…³é”®æ­¥éª¤ 1: Z-Score æ ‡å‡†åŒ– (è®©åˆ†æ•°åˆ†å¸ƒå¯¹é½ï¼Œä¸ä¾èµ–ç»å¯¹å€¼) ===
        #     # è¿™æ ·æ— è®ºæ˜¯ [0.2, 0.4] è¿˜æ˜¯ [0.8, 1.0]ï¼Œç›¸å¯¹å·®è·éƒ½èƒ½è¢«æ­£ç¡®æ•æ‰
        #     scores_mean = batch_scores.mean()
        #     scores_std = batch_scores.std() + 1e-6  # é˜²æ­¢é™¤ä»¥ 0
        #     z_scores = (batch_scores - scores_mean) / scores_std
            
        #     # === å…³é”®æ­¥éª¤ 2: æ¸©åº¦ç³»æ•° k (Temperature) ===
        #     # k = 1.0 : æ¸©å’ŒåŠ æƒï¼Œä¿ç•™ä½åˆ†æ ·æœ¬çš„å­¦ä¹ ä¿¡å· (æ¨èèµ·ç‚¹)
        #     # k = 2.0 : æ¿€è¿›åŠ æƒï¼Œå¼ºåŠ›å†²å‡» SOTA
        #     # k > 5.0 : æåº¦è´ªå©ªï¼Œå¯èƒ½å¯¼è‡´ Median ä¸‹é™ (è¿‘ä¼¼ Argmax)
        #     # å»ºè®®å…ˆè®¾ä¸º 2.0ï¼Œæ—¢èƒ½æ‹‰å¼€å·®è·ï¼Œåˆä¸ä¼šæŠŠä½åˆ†æ ·æœ¬æƒé‡æ€åˆ° 0
        #     k = 2.0 
        #     if Config.TASK_NAME == 'TFBind10-Exact-v0':
        #         k = 0.5
            
        #     # è®¡ç®— Softmax
        #     weights_softmax = torch.softmax(z_scores * k, dim=0)
            
        #     # === å…³é”®æ­¥éª¤ 3: é‡ç¼©æ”¾ (Rescaling) ===
        #     # Softmax çš„å’Œæ˜¯ 1ï¼Œæˆ‘ä»¬éœ€è¦ å’Œ = N (å³å¹³å‡æƒé‡ä¸º 1)
        #     # å¦åˆ™æ¢¯åº¦ä¼šæ¶ˆå¤±
        #     batch_size = len(batch_scores)
        #     weights = weights_softmax * batch_size
            
        #     # è½¬å› numpy ä¼ ç»™è®­ç»ƒå‡½æ•°
        #     weights_np = weights.cpu().numpy()
            
        #     # (å¯é€‰) æ‰“å°ä¸€ä¸‹æƒé‡çš„æå€¼ï¼Œçœ‹çœ‹æ˜¯ä¸æ˜¯å¤ªæç«¯
        #     if epoch % 10 == 0:
        #         print(f"  [Weight Debug] Min: {weights.min().item():.4f} | Max: {weights.max().item():.4f} | Mean: {weights.mean().item():.4f}")

        # else:
        #     weights_np = None
        if len(scores_array) > 0:
            batch_scores = torch.FloatTensor(scores_array).to(device)
            N = len(batch_scores)
            
            # === Rank-Based Weighting (åŸºäºæ’åçš„åŠ æƒ) ===
            # 1. è·å–æ’å (argsort ä¸¤æ¬¡å¯ä»¥å¾—åˆ°æ¯ä¸ªå…ƒç´ çš„æ’åç´¢å¼•)
            sorted_indices = torch.argsort(batch_scores)
            ranks = torch.zeros_like(sorted_indices, dtype=torch.float, device=device)
            ranks[sorted_indices] = torch.arange(N, device=device, dtype=torch.float)
            
            # 2. å½’ä¸€åŒ–æ’ååˆ° [0, 1] åŒºé—´
            normalized_ranks = ranks / (N - 1)  # Range: [0.0, 1.0]
            
            # 3. å¸¦æ¸©åº¦çš„ Softmax (k æ§åˆ¶è´ªå©ªç¨‹åº¦)
            k = 3.0 
            weights_softmax = torch.softmax(normalized_ranks * k, dim=0)
            
            # 4. é‡ç¼©æ”¾ (ä¿æŒ Sum = N)
            weights = weights_softmax * N
            weights_np = weights.cpu().numpy()
            
            if epoch % 10 == 0:
                print(f"  [Rank Weight] Min: {weights.min().item():.4f} | Max: {weights.max().item():.4f}")

        else:
            weights_np = None
        traj_gen_time = time.time() - traj_gen_start
        
        if len(trajs_array) == 0:
            print(f"Warning: No valid trajectories generated in epoch {epoch+1}")
            continue
        
        print(f"Generated {len(trajs_array)} trajectories from GP samples")
        print(f"  [â±ï¸ Time] GPåˆå§‹åŒ–: {gp_init_time:.2f}s | GPé‡‡æ ·: {sampling_time:.2f}s | è½¨è¿¹ç”Ÿæˆ: {traj_gen_time:.2f}s")
        
        # å¯¹è¿™æ‰¹è½¨è¿¹è¿›è¡ŒæµåŒ¹é…è®­ç»ƒæ›´æ–° (Rank-Based Weighting)
        train_start = time.time()
        avg_loss = train_cfm_step(cfm_model, trajs_array, optimizer, device, weights=weights_np)
        train_time = time.time() - train_start
        
        epoch_total_time = time.time() - epoch_start
        
        # ğŸ”§ æ›´æ–°å­¦ä¹ ç‡
        # scheduler.step()
        
        print(f"  [â±ï¸ Time] è®­ç»ƒ: {train_time:.2f}s | Epochæ€»è®¡: {epoch_total_time:.2f}s")
        
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/{Config.FM_EPOCHS} | Loss: {avg_loss:.4f} | Trajs: {len(trajs_array)}")
            # ä¿å­˜æ£€æŸ¥ç‚¹
            checkpoint_path = f"checkpoints/cfm_model_epoch_{epoch+1}.pt"
            os.makedirs("checkpoints", exist_ok=True)
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': cfm_model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
            }, checkpoint_path)
            print(f"  [ğŸ’¾ Checkpoint] Saved to {checkpoint_path}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = "checkpoints/cfm_model_final.pt"
    os.makedirs("checkpoints", exist_ok=True)
    torch.save({
        'epoch': Config.FM_EPOCHS,
        'model_state_dict': cfm_model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'input_dim': input_dim,
        'hidden_dim': Config.HIDDEN_DIM,
    }, final_model_path)
    print(f"\n[ğŸ’¾ Final Model] Saved to {final_model_path}")

    # 4. æ¨ç†ä¸ SOTA è¯„ä¼° (Q=128)ï¼ˆå®Œå…¨å¯¹é½ ROOT çš„æµ‹è¯•é€»è¾‘ï¼‰
    print(f"\n=== SOTA Evaluation (Highest-point, Q=128) ===")
    
    # å¯¹é½ ROOTï¼šä»å¾—åˆ†æœ€é«˜çš„ 128 ä¸ªæ ‡å‡†åŒ–æ ·æœ¬å‡ºå‘
    test_q = Config.NUM_TEST_SAMPLES
    
    # ä½¿ç”¨æ ‡å‡†åŒ–åçš„ y æ¥é€‰æ‹©é«˜åˆ†æ ·æœ¬ï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
    # y_train_norm æ˜¯ numpy arrayï¼Œæ‰€ä»¥è¿™é‡Œçš„ç´¢å¼•éƒ½æ˜¯ numpy
    sorted_indices = np.argsort(y_train_norm)
    high_indices = sorted_indices[-test_q:]
    
    # è·å–æ ‡å‡†åŒ–çš„é«˜åˆ†æ ·æœ¬ä½œä¸ºèµ·ç‚¹
    X_test_norm = X_train_norm[high_indices]
    y_test_start = y_train_norm[high_indices]
    
    print(f"Selected {test_q} highest samples as starting points")
    print(f"Starting scores (normalized): mean={np.mean(y_test_start):.4f}, max={np.max(y_test_start):.4f}")
    
    # ODE æ¨ç† (ä½¿ç”¨ Velocity Scaling è¿›è¡Œå¤–æ¨åŠ é€Ÿ)
    # velocity_scale > 1.0: åˆ©ç”¨ Rank Weighting è®­ç»ƒçš„é«˜è´¨é‡æ–¹å‘è¿›è¡Œå¤–æ¨
    opt_X_norm = inference_ode(cfm_model, X_test_norm, device, velocity_scale=1.5)
    
    # åæ ‡å‡†åŒ–ï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
    opt_X_denorm = opt_X_norm * std_x + mean_x
    
    # è¿˜åŸå½¢çŠ¶ä¾› task.predict æ‰“åˆ†ï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
    if task.is_discrete and logits_shape is not None:
        # ç¦»æ•£ä»»åŠ¡ï¼šéœ€è¦ reshape å› (N, L, V-1) çš„å½¢çŠ¶
        # ä½¿ç”¨æ•°æ®åŠ è½½æ—¶ä¿å­˜çš„ logits_shape ä¿¡æ¯
        opt_X_for_predict = opt_X_denorm.reshape(test_q, logits_shape[1], logits_shape[2])
        # åŸå§‹æ ·æœ¬ä¹Ÿéœ€è¦ç›¸åŒå¤„ç†
        original_X_denorm = X_test_norm * std_x + mean_x
        original_X_for_predict = original_X_denorm.reshape(test_q, logits_shape[1], logits_shape[2])
        
        print(f"[Discrete Task] Reshaped to Logits format: {opt_X_for_predict.shape}")
    else:
        # è¿ç»­ä»»åŠ¡ï¼šç›´æ¥ä½¿ç”¨
        opt_X_for_predict = opt_X_denorm
        original_X_for_predict = X_test_norm * std_x + mean_x
    
    # ä½¿ç”¨ Oracle è¯„ä¼°ï¼ˆä¸ ROOT ä¸€è‡´ï¼Œç›´æ¥ä¼ å…¥ numpy arrayï¼‰
    final_scores = task.predict(opt_X_for_predict).flatten()
    original_scores = task.predict(original_X_for_predict).flatten()
    
    # ä¸ ROOT å®Œå…¨å¯¹é½ï¼šä½¿ç”¨ç›¸åŒçš„ task_to_min, task_to_max, task_to_best å­—å…¸
    task_to_min = {'TFBind8-Exact-v0': 0.0, 'TFBind10-Exact-v0': -1.8585268, 'AntMorphology-Exact-v0': -386.90036, 'DKittyMorphology-Exact-v0': -880.4585}
    task_to_max = {'TFBind8-Exact-v0': 1.0, 'TFBind10-Exact-v0': 2.1287067, 'AntMorphology-Exact-v0': 590.24445, 'DKittyMorphology-Exact-v0': 340.90985}
    task_to_best = {'TFBind8-Exact-v0': 0.43929616, 'TFBind10-Exact-v0': 0.005328223, 'AntMorphology-Exact-v0': 165.32648, 'DKittyMorphology-Exact-v0': 199.36252}
    
    # è·å–ä»»åŠ¡çš„ min å’Œ max å€¼ï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
    oracle_y_min = task_to_min[Config.TASK_NAME]
    oracle_y_max = task_to_max[Config.TASK_NAME]
    
    # è®¡ç®—æ ‡å‡†åŒ–åˆ†æ•°ï¼ˆä¸ ROOT å®Œå…¨ä¸€è‡´ï¼‰
    # ROOT ä½¿ç”¨å…¬å¼: final_score = (high_true_scores - oracle_y_min) / (oracle_y_max - oracle_y_min)
    final_score_normalized = (torch.from_numpy(final_scores) - oracle_y_min) / (oracle_y_max - oracle_y_min)
    original_score_normalized = (torch.from_numpy(original_scores) - oracle_y_min) / (oracle_y_max - oracle_y_min)
    
    # è®¡ç®—ç™¾åˆ†ä½æ•°ï¼ˆä¸ ROOT å®Œå…¨ä¸€è‡´ï¼šä½¿ç”¨æ ‡å‡†åŒ–åçš„åˆ†æ•°ï¼‰
    percentiles = torch.quantile(final_score_normalized, torch.tensor([1.0, 0.8, 0.5]), interpolation='higher')
    p100_score = percentiles[0].item()
    p80_score = percentiles[1].item()
    p50_score = percentiles[2].item()
    
    # æ‰“å°åŸå§‹åˆ†æ•°åˆ†å¸ƒï¼ˆç”¨äºè°ƒè¯•ï¼‰
    final_scores_sorted = np.sort(final_scores)
    print(f"\n[Result] Final scores distribution (raw):")
    print(f"  Min: {final_scores_sorted[0]:.4f}")
    print(f"  Max: {final_scores_sorted[-1]:.4f}")
    print(f"  Mean: {np.mean(final_scores):.4f}")
    print(f"  Std: {np.std(final_scores):.4f}")
    
    # æ‰“å°æ ‡å‡†åŒ–åçš„åˆ†æ•°åˆ†å¸ƒ
    final_score_normalized_np = final_score_normalized.numpy()
    print(f"\n[Result] Normalized scores distribution (comparable with ROOT):")
    print(f"  Min: {np.min(final_score_normalized_np):.4f}")
    print(f"  Max: {np.max(final_score_normalized_np):.4f}")
    print(f"  Mean: {np.mean(final_score_normalized_np):.4f}")
    print(f"  Std: {np.std(final_score_normalized_np):.4f}")
    
    print("-" * 60)
    print(f"Original Mean (Top {test_q}, raw): {np.mean(original_scores):.4f}")
    print(f"Optimized Mean (Top {test_q}, raw): {np.mean(final_scores):.4f}")
    print(f"Improvement (raw):                   {np.mean(final_scores) - np.mean(original_scores):.4f}")
    print("-" * 60)
    print(f"Normalized 100th Percentile (Max):      {p100_score:.6f}")
    print(f"Normalized 80th Percentile:             {p80_score:.6f}")
    print(f"Normalized 50th Percentile (Median):    {p50_score:.6f}")
    print("-" * 60)
    print(f"[ROOT Comparable] These normalized percentiles are directly comparable with ROOT results")

if __name__ == "__main__":
    main()


#TF10 0.685  0.526  0.473
#Ant 0.965 0.847 0.712
#Dkitty 0.972 0.938 0.919
#TF8  0.986   0.839 0.675