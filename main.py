# main.py
import design_bench
import torch
import torch.optim as optim
import numpy as np

from src.config import Config
from src.utils import set_seed, Normalizer
from src.oracle import NTKOracle
from src.generator import GP, sampling_data_from_GP, generate_trajectories_from_GP_samples
from src.models import VectorFieldNet
from src.flow import train_cfm_step,train_cfm, inference_ode
import time
import os

def get_design_bench_data(task_name):
    """
    åŠ è½½å¹¶æ ‡å‡†åŒ– Design-Bench æ•°æ®ï¼Œæ”¯æŒç¦»æ•£ä»»åŠ¡è½¬æ¢
    å®Œå…¨å¯¹é½ ROOT çš„å¤„ç†æ–¹å¼
    """
    print(f"Loading task: {task_name}...")
    if task_name != 'TFBind10-Exact-v0':
        task = design_bench.make(task_name)
    else:
        # æ˜¾å­˜ä¼˜åŒ–ï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
        task = design_bench.make(task_name, dataset_kwargs={"max_samples": 10000})
    
    offline_x = task.x
    logits_shape = None  # ä¿å­˜ logits å½¢çŠ¶ä¿¡æ¯
    
    if task.is_discrete:
        # ROOT é£æ ¼ï¼šä½¿ç”¨ map_to_logits ä¿®æ”¹ task å†…éƒ¨çŠ¶æ€
        # è¿™æ · task.predict() æ‰èƒ½æ­£ç¡®å¤„ç† logits æ ¼å¼çš„æ•°æ®
        task.map_to_logits()
        offline_x = task.x  # ç°åœ¨ task.x å·²ç»æ˜¯ logits æ ¼å¼ (N, L, V-1)
        logits_shape = offline_x.shape  # ä¿å­˜å½¢çŠ¶ (N, L, V-1)
        offline_x = offline_x.reshape(offline_x.shape[0], -1)  # å±•å¹³ä¸º (N, L*(V-1))
        print(f"[æ•°æ®ç¼–ç ] ç¦»æ•£ä»»åŠ¡ï¼šå·²è°ƒç”¨ map_to_logitsï¼ŒLogits {logits_shape} -> å±•å¹³ {offline_x.shape}")
    else:
        print("[æ•°æ®ç¼–ç ] è¿ç»­ä»»åŠ¡ï¼šç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®")
    
    # è®¡ç®—ç»Ÿè®¡é‡ï¼ˆä¸ ROOT å®Œå…¨ä¸€è‡´ï¼‰
    mean_x = np.mean(offline_x, axis=0)
    std_x = np.std(offline_x, axis=0)
    std_x = np.where(std_x == 0, 1.0, std_x)  # ROOT ä½¿ç”¨ == 0ï¼Œä¸æ˜¯ < 1e-6
    offline_x_norm = (offline_x - mean_x) / std_x
    
    # å¤„ç† Yï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
    offline_y = task.y.reshape(-1)  # ROOT ä½¿ç”¨ reshape(-1)ï¼Œä¸æ˜¯ reshape(-1, 1)
    mean_y = np.mean(offline_y, axis=0)
    std_y = np.std(offline_y, axis=0)
    
    # æ´—ç‰Œæ•°æ®ï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
    shuffle_idx = np.random.permutation(offline_x.shape[0])
    offline_x_norm = offline_x_norm[shuffle_idx]
    offline_y = offline_y[shuffle_idx]
    
    # æ ‡å‡†åŒ– Y
    offline_y_norm = (offline_y - mean_y) / std_y
    
    return task, offline_x_norm, offline_y_norm, mean_x, std_x, mean_y, std_y, logits_shape

# main.py é¢„å¤„ç†é€»è¾‘
# def preprocess_trajectories(oracle, X_train_norm):
#     device = torch.device(Config.DEVICE if torch.cuda.is_available() else "cpu")
#     if os.path.exists(Config.TRAJECTORY_PATH):
#         print(f"Loading cached trajectories from {Config.TRAJECTORY_PATH}")
#         return np.load(Config.TRAJECTORY_PATH)['trajs']

#     print("=== Generating ALL long trajectories (GD reverse + GA) ===")
#     all_indices = np.arange(len(X_train_norm))
#     # æŒ‰ç…§ä½ çš„è¦æ±‚ï¼šç”¨å…¨é‡æ•°æ®ï¼Œæˆ–è€…è‡³å°‘ 10000 æ¡
#     sample_size = len(all_indices)
#     selected_idx = all_indices
#     # selected_idx = np.random.choice(all_indices, sample_size, replace=False)
    
#     all_valid = []
#     batch_size = 256
#     for i in range(0, sample_size, batch_size):
#         batch_x = X_train_norm[selected_idx[i : i + batch_size]]
#         trajs = generate_long_trajectories(oracle, batch_x, device)
#         all_valid.append(trajs)
#         print(f"Progress: {i + batch_size}/{sample_size}")
        
#     pool = np.concatenate(all_valid, axis=0)
#     np.savez_compressed(Config.TRAJECTORY_PATH, trajs=pool)
#     return pool

def main():
    # 0. åˆå§‹åŒ–ç¯å¢ƒ
    print(f"=== GGFM with ROOT GP Sampling: {Config.TASK_NAME} ===")
    set_seed(Config.SEED)
    device = torch.device(Config.DEVICE if torch.cuda.is_available() else "cpu")
    
    # 1. åŠ è½½å¹¶ç¼–ç æ•°æ®ï¼ˆå®Œå…¨å¯¹é½ ROOT çš„å¤„ç†æ–¹å¼ï¼‰
    task, X_train_norm, y_train_norm, mean_x, std_x, mean_y, std_y, logits_shape = get_design_bench_data(Config.TASK_NAME)
    
    # åŒæ­¥ Normalizer çŠ¶æ€
    x_normalizer = Normalizer(np.zeros((1, X_train_norm.shape[1])))
    x_normalizer.mean, x_normalizer.std, x_normalizer.device = mean_x, std_x, device
    
    # 2. è½¬æ¢ä¸º Tensor ä¾› GP ä½¿ç”¨ï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
    X_train_tensor = torch.FloatTensor(X_train_norm).to(device)
    # y_train_norm ç°åœ¨æ˜¯ (N,) å½¢çŠ¶ï¼Œéœ€è¦è½¬æ¢ä¸º (N, 1) ä¾› Oracle ä½¿ç”¨
    y_train_tensor = torch.FloatTensor(y_train_norm).reshape(-1, 1).to(device)
    
    # ä¿å­˜åŸå§‹ç»Ÿè®¡é‡ï¼ˆç”¨äºåæ ‡å‡†åŒ–ï¼‰
    mean_x_torch = torch.FloatTensor(mean_x).to(device)
    std_x_torch = torch.FloatTensor(std_x).to(device)
    mean_y_torch = torch.FloatTensor([mean_y]).to(device)
    std_y_torch = torch.FloatTensor([std_y]).to(device)
    
    # 3. åˆå§‹åŒ– GP è¶…å‚æ•°
    lengthscale = torch.tensor(Config.GP_INITIAL_LENGTHSCALE, device=device)
    variance = torch.tensor(Config.GP_INITIAL_OUTPUTSCALE, device=device)
    noise = torch.tensor(Config.GP_NOISE, device=device)
    mean_prior = torch.tensor(0.0, device=device)
    
    # 4. é€‰æ‹©ç”¨äº GP æ‹Ÿåˆçš„åˆå§‹ç‚¹ï¼ˆå®Œå…¨å¯¹é½ ROOTï¼‰
    if Config.GP_TYPE_INITIAL_POINTS == 'highest':
        # ROOT: å›ºå®š 1024 ä¸ªæ ·æœ¬ï¼Œæ¯æ¬¡å…¨é€‰ä½†é¡ºåºä¸åŒ
        best_indices = torch.argsort(y_train_tensor.view(-1))[-1024:]
        best_x = X_train_tensor[best_indices]
        print(f"[GP Init] Using top 1024 samples for GP sampling (ROOT style: same samples, different order each epoch)")
    elif Config.GP_TYPE_INITIAL_POINTS == 'lowest':
        best_indices = torch.argsort(y_train_tensor.view(-1))[:1024]
        best_x = X_train_tensor[best_indices]
        print(f"[GP Init] Using bottom 1024 samples for GP sampling")
    else:
        best_x = X_train_tensor
        print(f"[GP Init] Using all samples for GP sampling")
    
    # 5. åˆå§‹åŒ– Flow Matching ç½‘ç»œ
    input_dim = X_train_norm.shape[1]
    cfm_model = VectorFieldNet(input_dim, hidden_dim=Config.HIDDEN_DIM).to(device)
    optimizer = optim.Adam(cfm_model.parameters(), lr=Config.FM_LR)

    # --- æ ¸å¿ƒä¿®æ”¹ï¼šæ¯ä¸ª Epoch åŠ¨æ€é‡‡æ · GP å‡½æ•°ç”Ÿæˆè½¨è¿¹ ---
    print(f"=== Training: Dynamic GP Sampling ({Config.FM_EPOCHS} Epochs) ===")
    print(f"æ¯ä¸ª Epoch é‡‡æ · n_e = {Config.GP_NUM_FUNCTIONS} ä¸ª GP å‡½æ•°")
    print(f"æ¯ä¸ª GP å‡½æ•°é‡‡æ · {Config.GP_NUM_POINTS} ä¸ªé…å¯¹")
    print(f"æ€»è®¡å°†ç”Ÿæˆçº¦ {Config.GP_NUM_FUNCTIONS} Ã— {Config.FM_EPOCHS} = {Config.GP_NUM_FUNCTIONS * Config.FM_EPOCHS} ä¸ª GP å‡½æ•°")

    # y_train_norm å·²ç»æ˜¯ (N,) å½¢çŠ¶äº†ï¼Œä¸éœ€è¦ flatten
    y_scores_flat = y_train_norm

    for epoch in range(Config.FM_EPOCHS):
        # æ¯ä¸ª Epoch é‡æ–°é‡‡æ ·å…·æœ‰ä¸åŒè¶…å‚æ•°çš„ GP
        epoch_start = time.time()  # è®°å½• epoch å¼€å§‹æ—¶é—´
        print(f"\n=== Epoch {epoch+1}/{Config.FM_EPOCHS} ===")
        
        # æ„å»º GP æ¨¡å‹ï¼ˆTFBind8 ä½¿ç”¨éƒ¨åˆ†æ ·æœ¬ï¼Œä¸ ROOT ä¸€è‡´ï¼‰
        gp_init_start = time.time()
        if Config.TASK_NAME == 'TFBind8-Exact-v0':
            selected_fit_samples = torch.randperm(X_train_tensor.shape[0])[:Config.GP_NUM_FIT_SAMPLES]
            GP_Model = GP(
                device=device,
                x_train=X_train_tensor[selected_fit_samples],
                y_train=y_train_tensor[selected_fit_samples].view(-1),  # ç¡®ä¿æ˜¯ (N,) å½¢çŠ¶
                lengthscale=lengthscale,
                variance=variance,
                noise=noise,
                mean_prior=mean_prior
            )
        else:
            GP_Model = GP(
                device=device,
                x_train=X_train_tensor,
                y_train=y_train_tensor.view(-1),  # ç¡®ä¿æ˜¯ (N,) å½¢çŠ¶
                lengthscale=lengthscale,
                variance=variance,
                noise=noise,
                mean_prior=mean_prior
            )
        gp_init_time = time.time() - gp_init_start
        
        # ä» GP é‡‡æ · n_e = 8 ä¸ªå‡½æ•°ï¼Œæ¯ä¸ªå‡½æ•°ç”Ÿæˆ num_points ä¸ªé…å¯¹
        sampling_start = time.time()
        data_from_GP = sampling_data_from_GP(
            x_train=best_x,
            device=device,
            GP_Model=GP_Model,
            num_functions=Config.GP_NUM_FUNCTIONS,
            num_gradient_steps=Config.GP_NUM_GRADIENT_STEPS,
            num_points=Config.GP_NUM_POINTS,
            learning_rate=Config.GP_LEARNING_RATE,
            delta_lengthscale=Config.GP_DELTA_LENGTHSCALE,
            delta_variance=Config.GP_DELTA_VARIANCE,
            seed=epoch,  # ä½¿ç”¨ epoch ä½œä¸ºéšæœºç§å­ï¼Œç¡®ä¿æ¯ä¸ª epoch ä¸åŒ
            threshold_diff=Config.GP_THRESHOLD_DIFF,
            verbose=(epoch == 0)  # ç¬¬ä¸€ä¸ª epoch æ˜¾ç¤ºè¯¦ç»†è®¡æ—¶
        )
        sampling_time = time.time() - sampling_start
        
        # ä» GP é‡‡æ ·ç»“æœç”Ÿæˆè½¨è¿¹
        traj_gen_start = time.time()
        trajs_array = generate_trajectories_from_GP_samples(
            data_from_GP,
            device=device,
            num_steps=Config.GP_TRAJ_STEPS
        )
        traj_gen_time = time.time() - traj_gen_start
        
        if len(trajs_array) == 0:
            print(f"Warning: No valid trajectories generated in epoch {epoch+1}")
            continue
        
        print(f"Generated {len(trajs_array)} trajectories from GP samples")
        print(f"  [â±ï¸ Time] GPåˆå§‹åŒ–: {gp_init_time:.2f}s | GPé‡‡æ ·: {sampling_time:.2f}s | è½¨è¿¹ç”Ÿæˆ: {traj_gen_time:.2f}s")
        
        # å¯¹è¿™æ‰¹è½¨è¿¹è¿›è¡ŒæµåŒ¹é…è®­ç»ƒæ›´æ–°
        train_start = time.time()
        avg_loss = train_cfm_step(cfm_model, trajs_array, optimizer, device)
        train_time = time.time() - train_start
        
        epoch_total_time = time.time() - epoch_start
        
        print(f"  [â±ï¸ Time] è®­ç»ƒ: {train_time:.2f}s | Epochæ€»è®¡: {epoch_total_time:.2f}s")
        
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/{Config.FM_EPOCHS} | Loss: {avg_loss:.4f} | Trajs: {len(trajs_array)}")
            # ä¿å­˜æ£€æŸ¥ç‚¹
            checkpoint_path = f"checkpoints/cfm_model_epoch_{epoch+1}.pt"
            os.makedirs("checkpoints", exist_ok=True)
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': cfm_model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
            }, checkpoint_path)
            print(f"  [ğŸ’¾ Checkpoint] Saved to {checkpoint_path}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = "checkpoints/cfm_model_final.pt"
    os.makedirs("checkpoints", exist_ok=True)
    torch.save({
        'epoch': Config.FM_EPOCHS,
        'model_state_dict': cfm_model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'input_dim': input_dim,
        'hidden_dim': Config.HIDDEN_DIM,
    }, final_model_path)
    print(f"\n[ğŸ’¾ Final Model] Saved to {final_model_path}")

    # 4. æ¨ç†ä¸ SOTA è¯„ä¼° (Q=128)ï¼ˆå®Œå…¨å¯¹é½ ROOT çš„æµ‹è¯•é€»è¾‘ï¼‰
    print(f"\n=== SOTA Evaluation (Highest-point, Q=128) ===")
    
    # å¯¹é½ ROOTï¼šä»å¾—åˆ†æœ€é«˜çš„ 128 ä¸ªæ ‡å‡†åŒ–æ ·æœ¬å‡ºå‘
    test_q = Config.NUM_TEST_SAMPLES
    
    # ä½¿ç”¨æ ‡å‡†åŒ–åçš„ y æ¥é€‰æ‹©é«˜åˆ†æ ·æœ¬ï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
    # y_train_norm æ˜¯ numpy arrayï¼Œæ‰€ä»¥è¿™é‡Œçš„ç´¢å¼•éƒ½æ˜¯ numpy
    sorted_indices = np.argsort(y_train_norm)
    high_indices = sorted_indices[-test_q:]
    
    # è·å–æ ‡å‡†åŒ–çš„é«˜åˆ†æ ·æœ¬ä½œä¸ºèµ·ç‚¹
    X_test_norm = X_train_norm[high_indices]
    y_test_start = y_train_norm[high_indices]
    
    print(f"Selected {test_q} highest samples as starting points")
    print(f"Starting scores (normalized): mean={np.mean(y_test_start):.4f}, max={np.max(y_test_start):.4f}")
    
    # ODE æ¨ç†ï¼ˆæ·»åŠ  y æ¡ä»¶å’Œ CFGï¼‰
    # ä¸ ROOT å®Œå…¨å¯¹é½ï¼šä½¿ç”¨ Oracle ç†è®ºæœ€å¤§å€¼è€Œéæ•°æ®é›†åˆ†ä½æ•°ï¼
    
    opt_X_norm = inference_ode(cfm_model, X_test_norm, device)
    
    # åæ ‡å‡†åŒ–ï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
    opt_X_denorm = opt_X_norm * std_x + mean_x
    
    # è¿˜åŸå½¢çŠ¶ä¾› task.predict æ‰“åˆ†ï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
    if task.is_discrete and logits_shape is not None:
        # ç¦»æ•£ä»»åŠ¡ï¼šéœ€è¦ reshape å› (N, L, V-1) çš„å½¢çŠ¶
        # ä½¿ç”¨æ•°æ®åŠ è½½æ—¶ä¿å­˜çš„ logits_shape ä¿¡æ¯
        opt_X_for_predict = opt_X_denorm.reshape(test_q, logits_shape[1], logits_shape[2])
        # åŸå§‹æ ·æœ¬ä¹Ÿéœ€è¦ç›¸åŒå¤„ç†
        original_X_denorm = X_test_norm * std_x + mean_x
        original_X_for_predict = original_X_denorm.reshape(test_q, logits_shape[1], logits_shape[2])
        
        print(f"[Discrete Task] Reshaped to Logits format: {opt_X_for_predict.shape}")
    else:
        # è¿ç»­ä»»åŠ¡ï¼šç›´æ¥ä½¿ç”¨
        opt_X_for_predict = opt_X_denorm
        original_X_for_predict = X_test_norm * std_x + mean_x
    
    # ä½¿ç”¨ Oracle è¯„ä¼°ï¼ˆä¸ ROOT ä¸€è‡´ï¼Œç›´æ¥ä¼ å…¥ numpy arrayï¼‰
    final_scores = task.predict(opt_X_for_predict).flatten()
    original_scores = task.predict(original_X_for_predict).flatten()
    
    # è®¡ç®—æ ‡å‡†åŒ–åˆ†æ•°ï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
    # oracle_y_min, oracle_y_max = np.min(task.y), np.max(task.y)
    # final_score_norm = (final_scores - oracle_y_min) / (oracle_y_max - oracle_y_min)
    
    # è®¡ç®—ç™¾åˆ†ä½æ•°ï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
    final_scores_sorted = np.sort(final_scores)
    print(f"\n[Result] Final scores distribution:")
    print(f"  Min: {final_scores_sorted[0]:.4f}")
    print(f"  Max: {final_scores_sorted[-1]:.4f}")
    print(f"  Mean: {np.mean(final_scores):.4f}")
    print(f"  Std: {np.std(final_scores):.4f}")
    
    # ä½¿ç”¨ torch.quantile è®¡ç®—ç™¾åˆ†ä½æ•°ï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
    final_scores_tensor = torch.from_numpy(final_scores)
    percentiles = torch.quantile(final_scores_tensor, torch.tensor([1.0, 0.8, 0.5]), interpolation='higher')
    p100_score = percentiles[0].item()
    p80_score = percentiles[1].item()
    p50_score = percentiles[2].item()
    
    print("-" * 60)
    print(f"Original Mean (Top {test_q}): {np.mean(original_scores):.4f}")
    print(f"Optimized Mean (Top {test_q}): {np.mean(final_scores):.4f}")
    print(f"Improvement:                   {np.mean(final_scores) - np.mean(original_scores):.4f}")
    print("-" * 60)
    print(f"100th Percentile (Max):      {p100_score:.4f}")
    print(f"80th Percentile:             {p80_score:.4f}")
    print(f"50th Percentile (Median):    {p50_score:.4f}")
    print("-" * 60)

if __name__ == "__main__":
    main()