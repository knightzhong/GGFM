# main.py
import argparse
import design_bench
import torch
import torch.optim as optim
import numpy as np

from src.config import Config, load_config
from src.utils import set_seed, Normalizer
from src.oracle import NTKOracle
from src.generator import GP, sampling_data_from_GP, generate_trajectories_from_GP_samples,RFFGP
from src.models import VectorFieldNet
from src.flow import train_cfm_step,train_cfm, inference_ode
import time
import os


def parse_args():
    parser = argparse.ArgumentParser(description="GGFM training")
    parser.add_argument(
        "-c",
        "--config",
        type=str,
        default="configs/TfBind8_FlowMatching.yaml",
        help="Path to config yaml",
    )
    parser.add_argument(
        "-s",
        "--seed",
        type=int,
        default=None,
        help="Override random seed",
    )
    parser.add_argument(
        "--device",
        type=str,
        default=None,
        help="Override device (e.g. cuda, cpu)",
    )
    return parser.parse_args()


def resolve_config_path(config_path):
    if os.path.isabs(config_path) or os.path.exists(config_path):
        return config_path
    return os.path.join(os.path.dirname(__file__), config_path)

def get_design_bench_data(task_name):
    """
    åŠ è½½å¹¶æ ‡å‡†åŒ– Design-Bench æ•°æ®ï¼Œæ”¯æŒç¦»æ•£ä»»åŠ¡è½¬æ¢
    å®Œå…¨å¯¹é½ ROOT çš„å¤„ç†æ–¹å¼
    """
    print(f"Loading task: {task_name}...")
    if task_name != 'TFBind10-Exact-v0':
        task = design_bench.make(task_name)
    else:
        # æ˜¾å­˜ä¼˜åŒ–ï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
        task = design_bench.make(task_name, dataset_kwargs={"max_samples": 10000})
    
    offline_x = task.x
    logits_shape = None  # ä¿å­˜ logits å½¢çŠ¶ä¿¡æ¯
    
    if task.is_discrete:
        # ROOT é£æ ¼ï¼šä½¿ç”¨ map_to_logits ä¿®æ”¹ task å†…éƒ¨çŠ¶æ€
        # è¿™æ · task.predict() æ‰èƒ½æ­£ç¡®å¤„ç† logits æ ¼å¼çš„æ•°æ®
        task.map_to_logits()
        offline_x = task.x  # ç°åœ¨ task.x å·²ç»æ˜¯ logits æ ¼å¼ (N, L, V-1)
        logits_shape = offline_x.shape  # ä¿å­˜å½¢çŠ¶ (N, L, V-1)
        offline_x = offline_x.reshape(offline_x.shape[0], -1)  # å±•å¹³ä¸º (N, L*(V-1))
        print(f"[æ•°æ®ç¼–ç ] ç¦»æ•£ä»»åŠ¡ï¼šå·²è°ƒç”¨ map_to_logitsï¼ŒLogits {logits_shape} -> å±•å¹³ {offline_x.shape}")
    else:
        print("[æ•°æ®ç¼–ç ] è¿ç»­ä»»åŠ¡ï¼šç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®")
    
    # è®¡ç®—ç»Ÿè®¡é‡ï¼ˆä¸ ROOT å®Œå…¨ä¸€è‡´ï¼‰
    mean_x = np.mean(offline_x, axis=0)
    std_x = np.std(offline_x, axis=0)
    std_x = np.where(std_x == 0, 1.0, std_x)  # ROOT ä½¿ç”¨ == 0ï¼Œä¸æ˜¯ < 1e-6
    offline_x_norm = (offline_x - mean_x) / std_x
    
    # å¤„ç† Yï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
    offline_y = task.y.reshape(-1)  # ROOT ä½¿ç”¨ reshape(-1)ï¼Œä¸æ˜¯ reshape(-1, 1)
    mean_y = np.mean(offline_y, axis=0)
    std_y = np.std(offline_y, axis=0)
    
    # æ´—ç‰Œæ•°æ®ï¼ˆä¸ ROOT ä¸€è‡´ï¼‰
    shuffle_idx = np.random.permutation(offline_x.shape[0])
    offline_x_norm = offline_x_norm[shuffle_idx]
    offline_y = offline_y[shuffle_idx]
    
    # æ ‡å‡†åŒ– Y
    offline_y_norm = (offline_y - mean_y) / std_y
    
    return task, offline_x_norm, offline_y_norm, mean_x, std_x, mean_y, std_y, logits_shape

def main():
    args = parse_args()
    config_path = resolve_config_path(args.config)
    load_config(config_path)
    if args.seed is not None:
        Config.SEED = args.seed
    if args.device is not None:
        Config.DEVICE = args.device

    # 0. åˆå§‹åŒ–ç¯å¢ƒ
    print(f"=== GGFM with ROOT GP Sampling: {Config.TASK_NAME} ===")
    print(f"[Config] Using: {config_path}")
    set_seed(Config.SEED)
    device = torch.device(Config.DEVICE if torch.cuda.is_available() else "cpu")
    
    # 1. åŠ è½½å¹¶ç¼–ç æ•°æ®
    task, X_train_norm, y_train_norm, mean_x, std_x, mean_y, std_y, logits_shape = get_design_bench_data(Config.TASK_NAME)
    
    # åŒæ­¥ Normalizer çŠ¶æ€
    x_normalizer = Normalizer(np.zeros((1, X_train_norm.shape[1])))
    x_normalizer.mean, x_normalizer.std, x_normalizer.device = mean_x, std_x, device
    
    # 2. è½¬æ¢ä¸º Tensor
    X_train_tensor = torch.FloatTensor(X_train_norm).to(device)
    y_train_tensor = torch.FloatTensor(y_train_norm).reshape(-1, 1).to(device)
    
    # 3. åˆå§‹åŒ– GP è¶…å‚æ•°
    lengthscale = torch.tensor(Config.GP_INITIAL_LENGTHSCALE, device=device)
    variance = torch.tensor(Config.GP_INITIAL_OUTPUTSCALE, device=device)
    noise = torch.tensor(Config.GP_NOISE, device=device)
    
    # é¢„å…ˆå‡†å¤‡å¥½å…¨é‡ç´¢å¼•
    all_indices = torch.arange(X_train_tensor.shape[0], device=device)
    top_k_indices = torch.argsort(y_train_tensor.view(-1), descending=True)[:2000]
    
    # 4. åˆå§‹åŒ– GP æ¨¡å‹ (å›ºå®šå‚æ•°ï¼Œä»…åˆå§‹åŒ–ä¸€æ¬¡)
    print(f"[GP Init] Fitting GP once with fixed parameters...")
    GP_Model = GP(
        device=device,
        x_train=X_train_tensor, 
        y_train=y_train_tensor.view(-1),
        lengthscale=lengthscale, 
        variance=variance, 
        noise=noise,
        mean_prior=torch.tensor(0.0, device=device)
    )
    # è®¡ç®— Cholesky åˆ†è§£å¹¶å›ºå®šå‚æ•°
    GP_Model.set_hyper(lengthscale, variance)

    # --- [æ–°å¢] åœ¨è®­ç»ƒå¼€å§‹å‰ï¼Œä¸€æ¬¡æ€§é‡‡æ ·å¤šä¸ª GP functions ---
    K = Config.GP_NUM_FUNCTIONS
    fixed_gp_functions = GP_Model.sample_functions(
        num_functions=K,
        seed=Config.SEED
    )
    print(f"[GP] Sampled {K} fixed GP functions for FM training.")

    # 5. åˆå§‹åŒ– Flow Matching ç½‘ç»œ
    input_dim = X_train_norm.shape[1]
    cfm_model = VectorFieldNet(
        input_dim,
        hidden_dim=Config.HIDDEN_DIM,
        dropout=Config.DROPOUT,
    ).to(device)
    optimizer = optim.Adam(cfm_model.parameters(), lr=Config.FM_LR)
    
    # --- æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨ Langevin åŠ¨åŠ›å­¦é‡‡æ ·ç”Ÿæˆè½¨è¿¹ ---
    print(f"=== Training: Langevin GP Sampling ({Config.FM_EPOCHS} Epochs) ===")
    print(f"æ¯ä¸ª Epoch é‡‡æ · n_e = 1 ä¸ªå›ºå®š GP å‡½æ•°é›†åˆ (K={Config.GP_NUM_FUNCTIONS})")
    print(f"æ¯ä¸ª Epoch å°è¯•ç”Ÿæˆ {Config.GP_NUM_POINTS} ä¸ª Langevin è½¨è¿¹")

    for epoch in range(Config.FM_EPOCHS):
        epoch_start = time.time()
        print(f"\n=== Epoch {epoch+1}/{Config.FM_EPOCHS} ===")

        # æ„å»ºæ··åˆé‡‡æ ·æ± 
        num_high = int(Config.GP_NUM_POINTS // 2)
        idx_high = top_k_indices[torch.randperm(len(top_k_indices))[:num_high]]
        num_rand = Config.GP_NUM_POINTS - num_high
        idx_rand = all_indices[torch.randperm(len(all_indices))[:num_rand]]
        mixed_indices = torch.cat([idx_high, idx_rand])
        current_epoch_x = X_train_tensor[mixed_indices]
        
        # Langevin é‡‡æ ·
        sampling_start = time.time()
        data_from_GP = sampling_data_from_GP(
            x_train=current_epoch_x,
            device=device,
            GP_Model=GP_Model,
            gp_functions=fixed_gp_functions, # ä½¿ç”¨å›ºå®šå‡½æ•°é›†åˆ
            num_gradient_steps=Config.GP_NUM_GRADIENT_STEPS,
            num_points=Config.GP_NUM_POINTS,
            eta_min=Config.GP_ETA_MIN,
            eta_max=Config.GP_ETA_MAX,
            sigma_max=Config.GP_SIGMA_MAX,
            threshold_diff=Config.GP_THRESHOLD_DIFF,
            uncertainty_penalty=Config.GP_UNCERTAINTY_PENALTY,
            uncertainty_interval=Config.GP_UNCERTAINTY_INTERVAL,
            max_end_uncertainty=Config.GP_MAX_END_UNCERTAINTY,
            verbose=True # å¼€å¯è¯¦ç»†è¯Šæ–­ä¿¡æ¯
        )
        sampling_time = time.time() - sampling_start
        
        # ä» GP é‡‡æ ·ç»“æœç”Ÿæˆè½¨è¿¹
        traj_gen_start = time.time()
        trajs_array, scores_array = generate_trajectories_from_GP_samples(
            data_from_GP,
            device=device,
            num_steps=Config.GP_TRAJ_STEPS
        )
        
        # Rank-Based Weighting
        if len(scores_array) > 0:
            batch_scores = torch.FloatTensor(scores_array).to(device)
            N = len(batch_scores)
            sorted_indices = torch.argsort(batch_scores)
            ranks = torch.zeros_like(sorted_indices, dtype=torch.float, device=device)
            ranks[sorted_indices] = torch.arange(N, device=device, dtype=torch.float)
            normalized_ranks = ranks / (N - 1)
            k = 3.0 
            weights_softmax = torch.softmax(normalized_ranks * k, dim=0)
            weights = weights_softmax * N
            weights_np = weights.cpu().numpy()
            if epoch % 10 == 0:
                print(f"  [Rank Weight] Min: {weights.min().item():.4f} | Max: {weights.max().item():.4f}")
        else:
            weights_np = None
            
        traj_gen_time = time.time() - traj_gen_start
        
        if len(trajs_array) == 0:
            print(f"Warning: No valid trajectories generated in epoch {epoch+1}")
            continue
        
        print(f"Generated {len(trajs_array)} trajectories from GP samples")
        print(f"  [â±ï¸ Time] GPé‡‡æ ·: {sampling_time:.2f}s | è½¨è¿¹ç”Ÿæˆ: {traj_gen_time:.2f}s")
        
        # è®­ç»ƒæ›´æ–°
        train_start = time.time()
        avg_loss = train_cfm_step(cfm_model, trajs_array, optimizer, device, weights=weights_np)
        train_time = time.time() - train_start
        epoch_total_time = time.time() - epoch_start
        
        print(f"  [â±ï¸ Time] è®­ç»ƒ: {train_time:.2f}s | Epochæ€»è®¡: {epoch_total_time:.2f}s")
        print(f"Epoch {epoch+1}/{Config.FM_EPOCHS} | Loss: {avg_loss:.4f} | Trajs: {len(trajs_array)}")

        
        if (epoch + 1) % 10 == 0:
            checkpoint_path = f"checkpoints/cfm_model_epoch_{epoch+1}.pt"
            os.makedirs("checkpoints", exist_ok=True)
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': cfm_model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
            }, checkpoint_path)
            print(f"  [ğŸ’¾ Checkpoint] Saved to {checkpoint_path}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = "checkpoints/cfm_model_final.pt"
    os.makedirs("checkpoints", exist_ok=True)
    torch.save({
        'epoch': Config.FM_EPOCHS,
        'model_state_dict': cfm_model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'input_dim': input_dim,
        'hidden_dim': Config.HIDDEN_DIM,
    }, final_model_path)
    print(f"\n[ğŸ’¾ Final Model] Saved to {final_model_path}")

    # æ¨ç†ä¸ SOTA è¯„ä¼°
    print(f"\n=== SOTA Evaluation (Highest-point, Q=128) ===")
    test_q = Config.NUM_TEST_SAMPLES
    sorted_indices = np.argsort(y_train_norm)
    high_indices = sorted_indices[-test_q:]
    X_test_norm = X_train_norm[high_indices]
    y_test_start = y_train_norm[high_indices]
    
    print(f"Selected {test_q} highest samples as starting points")
    print(f"Starting scores (normalized): mean={np.mean(y_test_start):.4f}, max={np.max(y_test_start):.4f}")
    
    opt_X_norm = inference_ode(cfm_model, X_test_norm, device, velocity_scale=1.5)
    opt_X_denorm = opt_X_norm * std_x + mean_x
    
    if task.is_discrete and logits_shape is not None:
        opt_X_for_predict = opt_X_denorm.reshape(test_q, logits_shape[1], logits_shape[2])
        original_X_denorm = X_test_norm * std_x + mean_x
        original_X_for_predict = original_X_denorm.reshape(test_q, logits_shape[1], logits_shape[2])
    else:
        opt_X_for_predict = opt_X_denorm
        original_X_for_predict = X_test_norm * std_x + mean_x
    
    final_scores = task.predict(opt_X_for_predict).flatten()
    original_scores = task.predict(original_X_for_predict).flatten()
    
    task_to_min = {'TFBind8-Exact-v0': 0.0, 'TFBind10-Exact-v0': -1.8585268, 'AntMorphology-Exact-v0': -386.90036, 'DKittyMorphology-Exact-v0': -880.4585}
    task_to_max = {'TFBind8-Exact-v0': 1.0, 'TFBind10-Exact-v0': 2.1287067, 'AntMorphology-Exact-v0': 590.24445, 'DKittyMorphology-Exact-v0': 340.90985}
    
    oracle_y_min = task_to_min[Config.TASK_NAME]
    oracle_y_max = task_to_max[Config.TASK_NAME]
    
    final_score_normalized = (torch.from_numpy(final_scores) - oracle_y_min) / (oracle_y_max - oracle_y_min)
    percentiles = torch.quantile(final_score_normalized, torch.tensor([1.0, 0.8, 0.5]), interpolation='higher')
    
    print(f"\n[Result] Final scores distribution (raw):")
    print(f"  Max: {np.max(final_scores):.4f} | Mean: {np.mean(final_scores):.4f}")
    print("-" * 60)
    print(f"Normalized 100th Percentile (Max):      {percentiles[0].item():.6f}")
    print(f"Normalized 80th Percentile:             {percentiles[1].item():.6f}")
    print(f"Normalized 50th Percentile (Median):    {percentiles[2].item():.6f}")
    print("-" * 60)

if __name__ == "__main__":
    main()
